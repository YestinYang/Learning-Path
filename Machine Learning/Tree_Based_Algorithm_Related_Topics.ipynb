{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Algorithm Related Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "The way random forest trained is as below: [Quora](https://www.quora.com/What-is-the-out-of-bag-error-in-random-forests-What-does-it-mean-Whats-a-typical-value-if-any-Why-would-it-be-higher-or-lower-than-a-typical-value)\n",
    "\n",
    "1\\. Suppose our training data set is represented by T and suppose data set has M features (or attributes or variables). T = {(X1,y1), (X2,y2), ... (Xn, yn)} and Xi is input vector {xi1, xi2, ... xiM} and yi is the label (or output or class). \n",
    "\n",
    "2\\. **(bootstrap / bagging)** Suppose we decide to have S number of trees in our forest then we first create S datasets of \"same size as original\" created from random resampling of data in T with-replacement (n times for each dataset). This will result in {T1, T2, ... TS} datasets. Each of these is called a bootstrap dataset. Due to \"with-replacement\" every dataset Ti can have duplicate data records and Ti can be missing several data records from original datasets. This is called Bagging. \n",
    "\n",
    "3\\. **(train each tree with subspace of features)** Now, RF creates S trees and uses m (=sqrt(M) or =floor(lnM+1)) random subfeatures out of M possible features to create any tree. This is called random subspace method. \n",
    "\n",
    "4\\. **(voting for final result)** So for each Ti bootstrap dataset you create a tree Ki. If you want to classify some input data D = {x1, x2, ..., xM} you let it pass through each tree and produce S outputs (one for each tree) which can be denoted by Y = {y1, y2, ..., ys}. Final prediction is a majority vote on this set. \n",
    "\n",
    "\n",
    "### Bootstrap Sampling (Bagging)\n",
    "\n",
    "Bootstrap sampling is the fundamental idea of random forest.\n",
    "\n",
    "**How bootstrap datasets are created?**\n",
    "\n",
    "- Randomly drawn from original dataset with replacement, to create a new dataset as same size as original dataset. It means that drawn a sample out of original dataset, and put it back before drawn the next one. In other words, each drawning is from the exact original dataset without lossing any sample.\n",
    "\n",
    "- Approximatly 1/3 (~1/e) of the samples in original dataset is left out from corresponding weak learner training, which is called out-of-bag samples.\n",
    "\n",
    "<img src=\"https://www.arb.ca.gov/research/weekendeffect/CARB041300/img012.gif\" alt=\"title\" >\n",
    "\n",
    "\n",
    "**How to use the out-of-bag dataset to evaluate the generalization ability of trained model?**  ([reference slides](http://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf))\n",
    "\n",
    "- For each single estimator, there is one bootstrap dataset and corresponding out-of-bag dataset. This oob dataset is just like the validation set of its estimator, which can be used to evaluate the generalization ability (show as slide one below).\n",
    "- For the generalization ability of the whole random forest model, it can be acheived by averaging among all the oob error of each single weak learner. (shown as silde two below)\n",
    "- Therefore, there is no need to use cross validation on random forest for hyperparameter searching, as stated in [scikit-learn documentation](http://scikit-learn.org/stable/modules/grid_search.html#out-of-bag-estimates). But still need test set to verify the whole model.\n",
    "    - (in scikit-learn) Use [ParameterGrid](https://stackoverflow.com/questions/34624978/is-there-easy-way-to-grid-search-without-cross-validation-in-python) instead of GridSearchCV and RandomizedSearchCV to avoid CV\n",
    "\n",
    "<img src=\"../img/oob_error.png\" alt=\"title\" >\n",
    "<img src=\"../img/oob_error2.png\" alt=\"title\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
