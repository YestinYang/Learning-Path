{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Twitter Data Wrangling\n",
    "\n",
    "This project is to gather the data from [WeRateDogs (@dog_rates)](https://twitter.com/dog_rates), then assess and clean the data for further exploratory data analysis, and also training a recurrent neural network to identify the species of dogs.\n",
    "\n",
    "The project focuses on below targets:\n",
    "1. Keep the original ranks from WeRateDogs only, not the ones from retweet \n",
    "2. At least 8 quality issues and 2 tidiness issues\n",
    "3. Including merge data table to achieve tidiness target\n",
    "4. Be noticed that numerator sometimes is larger than denominator in this dataset, and that is the way WeRateDogs runs\n",
    "\n",
    "The process of this project is as below:\n",
    "1. Data wrangling, including cleaning, assessing and cleaning in programmatic ways\n",
    "2. Store, analyze and visualize the processed data\n",
    "3. Summary the first two steps in a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "There are three files to be gathered:\n",
    "1. Download twitter data of WeRateDogs from [GitHub repo](https://github.com/udacity/new-dand-advanced-china/tree/master/数据清洗/WeRateDogs项目)\n",
    "2. Download programmatically for the image recognition result of dog species, in above repo\n",
    "3. Gather retweet_count and favorite_count via `Tweepy` API, and combine them into a JSON format txt file, each line representing a single record, at least including `tweet ID`, `retweet_count` and `favorite_count` information \n",
    "\n",
    "After gathering, import all into separated `pandas.DataFrame` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2356 entries, 0 to 2355\n",
      "Data columns (total 17 columns):\n",
      "tweet_id                      2356 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2356 non-null object\n",
      "source                        2356 non-null object\n",
      "text                          2356 non-null object\n",
      "retweeted_status_id           181 non-null float64\n",
      "retweeted_status_user_id      181 non-null float64\n",
      "retweeted_status_timestamp    181 non-null object\n",
      "expanded_urls                 2297 non-null object\n",
      "rating_numerator              2356 non-null int64\n",
      "rating_denominator            2356 non-null int64\n",
      "name                          2356 non-null object\n",
      "doggo                         2356 non-null object\n",
      "floofer                       2356 non-null object\n",
      "pupper                        2356 non-null object\n",
      "puppo                         2356 non-null object\n",
      "dtypes: float64(4), int64(3), object(10)\n",
      "memory usage: 313.0+ KB\n"
     ]
    }
   ],
   "source": [
    "##### 1. Import existed dataset, including tweet ID #####\n",
    "twitter_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "twitter_archive_enhanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2075 entries, 0 to 2074\n",
      "Data columns (total 12 columns):\n",
      "tweet_id    2075 non-null int64\n",
      "jpg_url     2075 non-null object\n",
      "img_num     2075 non-null int64\n",
      "p1          2075 non-null object\n",
      "p1_conf     2075 non-null float64\n",
      "p1_dog      2075 non-null bool\n",
      "p2          2075 non-null object\n",
      "p2_conf     2075 non-null float64\n",
      "p2_dog      2075 non-null bool\n",
      "p3          2075 non-null object\n",
      "p3_conf     2075 non-null float64\n",
      "p3_dog      2075 non-null bool\n",
      "dtypes: bool(3), float64(3), int64(2), object(4)\n",
      "memory usage: 152.1+ KB\n"
     ]
    }
   ],
   "source": [
    "##### 2. Programmatically download image recoginition result ######\n",
    "image_predictions = pd.read_csv('https://raw.githubusercontent.com/udacity/new-dand-advanced-china/master/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/WeRateDogs%E9%A1%B9%E7%9B%AE/image-predictions.tsv',\n",
    "                               sep='\\t')\n",
    "image_predictions.to_csv('image-predictions.tsv')\n",
    "image_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3. Info from Twitter API via tweepy #####\n",
    "### Create tweepy instance for Twitter API ###\n",
    "# consumer_key = ''\n",
    "# consumer_secret = ''\n",
    "# access_token = ''\n",
    "# access_token_secret = ''\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Looping to download all info and store in a list instance ###\n",
    "# tweet_json = []\n",
    "# tweet_error = {}\n",
    "# for tweet_id in twitter_archive_enhanced.tweet_id:\n",
    "#     try:\n",
    "#         tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "#         tweet_json.append(tweet._json)\n",
    "#     except Exception as e:\n",
    "#         tweet_error[tweet_id] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 343\n"
     ]
    }
   ],
   "source": [
    "### Re-download the error part ###\n",
    "# tweet_error_2nd = {}\n",
    "# for tweet_id in tweet_error.keys():\n",
    "#     try:\n",
    "#         tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "#         tweet_json.append(tweet._json)\n",
    "#     except Exception as e:\n",
    "#         tweet_error_2nd[tweet_id] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{704761120771465216: tweepy.error.TweepError(\"Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=60)\"),\n",
       " 775096608509886464: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 802247111496568832: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 827228250799742977: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 837012587749474308: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 842892208864923648: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 845459076796616705: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 861769973181624320: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 866816280283807744: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 869988702071779329: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 873697596434513921: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}]),\n",
       " 888202515573088257: tweepy.error.TweepError([{'code': 144,\n",
       "                           'message': 'No status found with that ID.'}])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet_error_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From above, there are 11 tweet_id are unable to find corresponding info ###\n",
    "### Re-download the last one ###\n",
    "# tweet = api.get_status('704761120771465216', tweet_mode='extended')\n",
    "# tweet_json.append(tweet._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store result in txt file ###\n",
    "# with open('tweet_json_YY.txt', 'w') as output:  \n",
    "#     for i in range(len(tweet_json)):\n",
    "#         json.dump(tweet_json[i], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import tweet_json.txt to pandas.DataFrame ###\n",
    "# Only include tweet ID, retweet_count and favorite_count #\n",
    "json_str = []\n",
    "with open('tweet_json.txt', encoding='utf-8') as json_file:\n",
    "    for i in range(2352):   # there are 2352 rows in txt file got from visual inspection\n",
    "        tweet = json_file.readline()\n",
    "        json_str.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform string read from txt to dic, and store in list to create DataFrame #\n",
    "json_list = []\n",
    "for i in range(len(json_str)):\n",
    "    json_list.append(json.loads(json_str[i]))\n",
    "\n",
    "tweet_json = pd.DataFrame(json_list)[['id', 'retweet_count', 'favorite_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2352 entries, 0 to 2351\n",
      "Data columns (total 3 columns):\n",
      "id                2352 non-null int64\n",
      "retweet_count     2352 non-null int64\n",
      "favorite_count    2352 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 55.2 KB\n"
     ]
    }
   ],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv for visual assessment in next step #\n",
    "tweet_json.to_csv('tweet_json.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Collection\n",
    "\n",
    "Corresponding to three targets,\n",
    "1. Existed file was loaded via `pandas.read_csv` from local path, storing as `twitter_archive_enhanced` dataframe\n",
    "2. Image recognition result was downloaded via `pandas.read_csv` from url, storing as `image_predictions` dataframe\n",
    "3. Tweet info downloaded via tweepy was stored in `tweet_json_YY.txt`, to be distinguishable from the provided `tweet_json.txt` . However, to be eaiser for project reivew, `tweet_json.txt` was still used to generate `tweet_json` dataframe for the rest of project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Assessing\n",
    "\n",
    "Accomplish at least 8 quality issues and 2 tidiness issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                        int64\n",
       "in_reply_to_status_id         float64\n",
       "in_reply_to_user_id           float64\n",
       "timestamp                      object\n",
       "source                         object\n",
       "text                           object\n",
       "retweeted_status_id           float64\n",
       "retweeted_status_user_id      float64\n",
       "retweeted_status_timestamp     object\n",
       "expanded_urls                  object\n",
       "rating_numerator                int64\n",
       "rating_denominator              int64\n",
       "name                           object\n",
       "doggo                          object\n",
       "floofer                        object\n",
       "pupper                         object\n",
       "puppo                          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_archive_enhanced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of items in a cell is 8\n"
     ]
    }
   ],
   "source": [
    "##### Check the max number of items in expanded_urls #####\n",
    "urls_split = twitter_archive_enhanced_clean.expanded_urls.str.strip().str.split(',')\n",
    "\n",
    "url_count = 1\n",
    "missing_cell = []\n",
    "for i in range(len(urls_split)):\n",
    "    try:\n",
    "        if len(urls_split[i])>url_count:\n",
    "            url_count = len(urls_split[i])\n",
    "    except Exception as e:\n",
    "        missing_cell.append(i)\n",
    "print ('Max number of items in a cell is ' + str(url_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality\n",
    "#### Table `twitter_archive_enhanced` \n",
    "- column `rating_denominator` has cell not equaling to 10. Some are due to multiple dogs in the picture, and some are due to wrong extraction from text. *(Validity and Consistency)*\n",
    "- column `name` has 55 values equaling to 'a', and another words like 'an', 'actually' etc, but actually there is no dog names in text *(Validity)*\n",
    "- column `source` is recored as html element but the useful part is only the content *(Validity)*\n",
    "- columns `timestamp` and `retweeted_status_timestamp` have '+0000' after all time but meaningless\n",
    "- because we only consider the tweets with original rates and pictures, retweet and tweet without pictures should be removed\n",
    "- columns `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id` and `retweeted_status_user_id` should be int64 dtype\n",
    "- columns `timestamp` and `retweeted_status_timestamp` should be datetime dtype\n",
    "- all 'None' values should be set as missing values using NaN\n",
    "\n",
    "#### Table `image_predicitons` \n",
    "- columns `p1`, `p2`, `p3` have cells starting with either upper case or lower case letter *(Consistency)*\n",
    "\n",
    "### Tidiness\n",
    "- (`twitter_archive_enhanced`) column `expanded_urls` has some cells with twitter urls and external urls, or duplicated urls\n",
    "- (`twitter_archive_enhanced`) columns `doggo`, `floofer`, `pupper` and `puppo` is actually one variable\n",
    "- table `tweet_json` and `image_predictions` are parts of `twitter_archive_enhanced` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Accomplish at least 8 quality issues and 2 tidiness issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Dummy read from file #####\n",
    "twitter_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "image_predictions = pd.read_csv('image-predictions.tsv', index_col=0)\n",
    "tweet_json = pd.read_csv('tweet_json.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Copy original data to be cleaned ######\n",
    "twitter_archive_enhanced_clean = twitter_archive_enhanced.copy()\n",
    "image_predictions_claen = image_predictions.copy()\n",
    "tweet_json_clean = tweet_json.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no completeness issues, so start with tidiness issues.\n",
    "\n",
    "### Tidiness\n",
    "\n",
    "#### 1. (twitter_archive_enhanced) column `expanded_urls` has some cells with twitter urls and external urls, or duplicated urls\n",
    "\n",
    "##### Define 1\n",
    "1. Split urls by comma, remove duplicated urls\n",
    "2. Filter out twitter urls and outter urls into separated columns\n",
    "\n",
    "##### Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of items in a cell is 2\n"
     ]
    }
   ],
   "source": [
    "### Split out urls as items in list ###\n",
    "urls_split = twitter_archive_enhanced_clean.expanded_urls.str.strip().str.split(',')\n",
    "\n",
    "### Remove duplicated urls ###\n",
    "missing_cell = []\n",
    "for i in range(len(urls_split)):\n",
    "    try:\n",
    "        urls_split[i] = list(set(urls_split[i]))\n",
    "    except Exception as e:\n",
    "        missing_cell.append(i)\n",
    "        \n",
    "##### Check the max number of items in expanded_urls after cleaning #####\n",
    "url_count = 1\n",
    "missing_cell = []\n",
    "for i in range(len(urls_split)):\n",
    "    try:\n",
    "        if len(urls_split[i])>url_count:\n",
    "            url_count = len(urls_split[i])\n",
    "    except Exception as e:\n",
    "        missing_cell.append(i)\n",
    "print ('Max number of items in a cell is ' + str(url_count))\n",
    "\n",
    "##### Split out twitter url and outter url #####\n",
    "regex = re.compile('https://twitter.*')\n",
    "twitter_urls = []\n",
    "outter_urls = []\n",
    "\n",
    "for i in range(len(urls_split)):\n",
    "    \n",
    "    if urls_split[i] != urls_split[i]:\n",
    "        twitter_urls.append(np.nan)\n",
    "        outter_urls.append(np.nan)\n",
    "    \n",
    "    elif list(filter(regex.match, urls_split[i])):\n",
    "        t_url = list(filter(regex.match, urls_split[i]))\n",
    "        twitter_urls.append(t_url[0])\n",
    "        outter_urls.append(np.nan)\n",
    "    else:\n",
    "        twitter_urls.append(np.nan)\n",
    "        outter_urls.append(urls_split[i][0])\n",
    "\n",
    "### Generate new columns and append on original dataframe ###\n",
    "clean_url_dict = {'twitter_urls': twitter_urls,\n",
    "                  'outter_urls': outter_urls}\n",
    "clean_urls = pd.DataFrame(clean_url_dict)\n",
    "\n",
    "### Drop original column and merge new columns to twitter_archive_enhanced_clean ###\n",
    "twitter_archive_enhanced_clean.drop('expanded_urls', axis=1, inplace=True)\n",
    "twitter_archive_enhanced_clean = pd.concat([twitter_archive_enhanced_clean, clean_urls], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://twitter.com/dog_rates/status/892420643...\n",
       "1    https://twitter.com/dog_rates/status/892177421...\n",
       "2    https://twitter.com/dog_rates/status/891815181...\n",
       "3    https://twitter.com/dog_rates/status/891689557...\n",
       "4    https://twitter.com/dog_rates/status/891327558...\n",
       "Name: twitter_urls, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_archive_enhanced_clean.twitter_urls.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308                              NaN\n",
       "309                              NaN\n",
       "310    https://vine.co/v/5W2Dg3XPX7a\n",
       "311                              NaN\n",
       "312                              NaN\n",
       "Name: outter_urls, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_notnull = twitter_archive_enhanced_clean.outter_urls.first_valid_index()\n",
    "twitter_archive_enhanced_clean.outter_urls.iloc[first_notnull-2:first_notnull+3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### 2. (twitter_archive_enhanced) columns doggo, floofer, pupper and puppo is actually one variable\n",
    "\n",
    "##### Define 2\n",
    "\n",
    "1. Combine four stage columns\n",
    "2. Manual update cell with multiple stage info by understanding each tweet\n",
    "\n",
    "##### Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Combine four stage columns into one list #####\n",
    "stage = twitter_archive_enhanced_clean[['doggo', 'floofer', 'pupper', 'puppo']]\n",
    "\n",
    "dogs_list = []\n",
    "for i in range(len(twitter_archive_enhanced_clean)):\n",
    "    dogs = list(set(stage.iloc[i].values))\n",
    "    if len(dogs)>1:\n",
    "        dogs.remove('None')\n",
    "    dogs_list.append(dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of single cell: 2\n"
     ]
    }
   ],
   "source": [
    "##### Manual handle cell with double dogs #####\n",
    "### Return a index list of cell with double dogs ###\n",
    "multi_dogs = []\n",
    "length = 1\n",
    "for i in range(len(dogs_list)):\n",
    "    if len(dogs_list[i])>1:\n",
    "        multi_dogs.append(i)\n",
    "        if len(dogs_list[i])>length:\n",
    "            length = len(dogs_list[i])\n",
    "print ('Maximum length of single cell: '+str(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read tweets and understand their real meaning ###\n",
    "dogs_list[multi_dogs[0]] = ['puppo']  # 191\n",
    "dogs_list[multi_dogs[1]] = ['floofer']  # 200\n",
    "dogs_list[multi_dogs[2]] = ['pupper']  # 460\n",
    "dogs_list[multi_dogs[3]] = dogs_list[multi_dogs[3]]  # two dogs in pic 531\n",
    "dogs_list[multi_dogs[4]] = dogs_list[multi_dogs[4]]  # two dogs in pic 565\n",
    "dogs_list[multi_dogs[5]] = ['pupper']  # 575\n",
    "dogs_list[multi_dogs[6]] = ['doggo']  # 705\n",
    "dogs_list[multi_dogs[7]] = dogs_list[multi_dogs[7]]  # two dogs in pic 733\n",
    "dogs_list[multi_dogs[8]] = dogs_list[multi_dogs[8]]  # two dogs in pic 778 RT\n",
    "dogs_list[multi_dogs[9]] = dogs_list[multi_dogs[9]]  # two dogs in pic 822 RT\n",
    "dogs_list[multi_dogs[10]] = dogs_list[multi_dogs[10]]  # two dogs in pic 889\n",
    "dogs_list[multi_dogs[11]] = ['None']  # 956\n",
    "dogs_list[multi_dogs[12]] = dogs_list[multi_dogs[12]]  # two dogs in pic 1063\n",
    "dogs_list[multi_dogs[13]] = dogs_list[multi_dogs[13]]  # two dogs in pic 1113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create new columns and update original DataFrame ######\n",
    "dogs_dict = {'dog_stage': list(\",\".join(map(str,i)) for i in dogs_list)}\n",
    "dog_stage = pd.DataFrame(dogs_dict)\n",
    "twitter_archive_enhanced_clean.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1, inplace=True)\n",
    "twitter_archive_enhanced_clean = pd.concat([twitter_archive_enhanced_clean, dog_stage], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10     None\n",
       "11     None\n",
       "12    puppo\n",
       "13     None\n",
       "14    puppo\n",
       "15     None\n",
       "Name: dog_stage, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_archive_enhanced_clean.dog_stage.iloc[10:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### 3. table `tweet_json` and `image_predictions` are parts of `twitter_archive_enhanced`\n",
    "\n",
    "##### Define 3\n",
    "\n",
    "1. Join `retweet_count` and `favorite_count` from `tweet_json` to `twitter_archive_enhanced`\n",
    "2. Join dog species with highest probability of prediciton from `image_predicitons` to `twitter_archive_enhanced`\n",
    "\n",
    "##### Code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Join with tweet_json #####\n",
    "twitter_archive_enhanced_clean = pd.merge(twitter_archive_enhanced_clean,\n",
    "                                          tweet_json,\n",
    "                                          left_on='tweet_id', right_on='id',\n",
    "                                          how='left')\n",
    "twitter_archive_enhanced_clean.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Join with image_prediction #####\n",
    "### Select out highest prob dog prediction ###\n",
    "tweet_id = []\n",
    "species_list = []\n",
    "species_prob_list = []\n",
    "\n",
    "for i in range(len(image_predictions)):\n",
    "    \n",
    "    tweet_id.append(image_predictions.tweet_id[i])\n",
    "    \n",
    "    if image_predictions.p1_dog[i]==True:\n",
    "        species_list.append(image_predictions.p1[i])\n",
    "        species_prob_list.append(image_predictions.p1_conf[i])\n",
    "    \n",
    "    elif image_predictions.p2_dog[i]==True:\n",
    "        species_list.append(image_predictions.p2[i])\n",
    "        species_prob_list.append(image_predictions.p2_conf[i])\n",
    "    \n",
    "    elif image_predictions.p3_dog[i]==True:\n",
    "        species_list.append(image_predictions.p3[i])\n",
    "        species_prob_list.append(image_predictions.p3_conf[i])\n",
    "    \n",
    "    else:\n",
    "        species_list.append(np.nan)\n",
    "        species_prob_list.append(np.nan)\n",
    "        \n",
    "### Create DataFrame and join ###\n",
    "image_pred_dict = {'tweet_id': tweet_id,\n",
    "                   'species': species_list,\n",
    "                   'species_prob': species_prob_list}\n",
    "image_pred = pd.DataFrame(image_pred_dict)\n",
    "twitter_archive_enhanced_clean = pd.merge(twitter_archive_enhanced_clean,\n",
    "                                          image_pred,\n",
    "                                          left_on='tweet_id', right_on='tweet_id',\n",
    "                                          how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   NaN\n",
       "1             Chihuahua\n",
       "2             Chihuahua\n",
       "3    Labrador_retriever\n",
       "4                basset\n",
       "Name: species, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_archive_enhanced_clean.species.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1    0.323581\n",
       "2    0.716012\n",
       "3    0.168086\n",
       "4    0.555712\n",
       "Name: species_prob, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_archive_enhanced_clean.species_prob.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage and Visualization\n",
    "\n",
    "1. Store the processed data as `twitter_archive_master.csv`\n",
    "2. Illustrate at least 3 intuitions from data analysis and 1 visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumamry Report\n",
    "\n",
    "1. 300-to-600-word report for internal assessment to complete project, about how the whole project was done, saved as `wrangle_report.pdf` \n",
    "2. ~250-word report for external demonstration like blog post, saved as `act_report.pdf` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
