{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "\n",
    "tags: binary classification, accuracy\n",
    "\n",
    "## Features Cleansing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import train data table\n",
    "# PassengerID as index\n",
    "train = pd.read_csv('train.csv', index_col='PassengerId')\n",
    "X = train.drop('Survived', axis=1)\n",
    "y = train.Survived\n",
    "\n",
    "pred = pd.read_csv('test.csv', index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare',\n",
      "       'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "PassengerId\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "Name: Survived, dtype: int64\n",
      "Index(['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare',\n",
      "       'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Check columns are correct\n",
    "print(X.columns)\n",
    "print(y.head(3))\n",
    "print(pred.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Name, Ticket Number and Cabin with less predictive\n",
    "X.drop(columns=['Name','Ticket','Cabin'], inplace=True)\n",
    "pred.drop(columns=['Name','Ticket','Cabin'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Age column\n",
    "# 1. add in new column to indicate estimated age\n",
    "# 2. remove 0.5 from estimated age\n",
    "\n",
    "def Insert_estAge(df_list):\n",
    "    for df in df_list:\n",
    "        if 'estAge' in df.columns:\n",
    "            continue\n",
    "        else:\n",
    "            df.insert(df.columns.tolist().index('Age')+1, 'estAge', 0)\n",
    "            df.loc[(df.Age/0.5)%2 == 1, 'estAge']=1\n",
    "            df.loc[(df.Age/0.5)%2 == 1, 'Age'] -= 0.5\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'estAge', 'SibSp', 'Parch', 'Fare', 'Embarked'], dtype='object')\n",
      "Index(['Pclass', 'Sex', 'Age', 'estAge', 'SibSp', 'Parch', 'Fare', 'Embarked'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Insert_estAge([X, pred])\n",
    "\n",
    "print(X.columns)\n",
    "print(pred.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass        0\n",
      "Sex           0\n",
      "Age         177\n",
      "estAge        0\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "Embarked      2\n",
      "dtype: int64\n",
      "Pclass       0\n",
      "Sex          0\n",
      "Age         86\n",
      "estAge       0\n",
      "SibSp        0\n",
      "Parch        0\n",
      "Fare         1\n",
      "Embarked     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Fill missing data\n",
    "# check missing value of dataframe\n",
    "print(X.isnull().sum())\n",
    "print(pred.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. define a class imputing numeric feature with median, and categorical feature with mode\n",
    "# 1. combine both dataframes\n",
    "# 2. impute median for numeric features and mode for categorical features\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # use list comprehensions to create a pandas series of fill value\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].mode()[0]\n",
    "                               if X[c].dtype == np.dtype('O')\n",
    "                               else X[c].median()\n",
    "                               for c in X],\n",
    "                              index=X.columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine and impute\n",
    "\n",
    "whole_df = X.append(pred)\n",
    "\n",
    "imp = DataFrameImputer()\n",
    "imp_whole_df = imp.fit_transform(whole_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform catagorical data into one-hot\n",
    "imp_whole_df = pd.get_dummies(imp_whole_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split out to be imp_X and imp_pred\n",
    "\n",
    "imp_X = imp_whole_df.loc[:X.shape[0], :].copy()\n",
    "imp_pred = imp_whole_df.loc[X.shape[0]+1:, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick and Dirty Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split train / validation / test set from imp_X\n",
    "\"\"\"\n",
    "In this dirty stage, assume the distribution of imp_X and imp_pred (actual target) is the same,\n",
    "so that just randomly select 30% of imp_X as test set,\n",
    "and use k-fold cross-validation in the other 70%.\n",
    "(Reminder: if distributions are different, need to carefully select validation\n",
    "and test set from imp_X, to confirm that both distribution is same as imp_pred.)\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(imp_X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consider TPOT(AutoML) to get fast result\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "pipeline_optimizer = TPOTClassifier(generations=50,\n",
    "                                    population_size=50,\n",
    "                                    cv=10,\n",
    "                                    scoring='accuracy',\n",
    "                                    early_stop=5,\n",
    "                                    verbosity=2,\n",
    "                                    n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.8219817346998732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.8252092199474536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.831479942585178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.831479942585178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.836319491660581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.836319491660581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.836319491660581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.836319491660581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.836319491660581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 0.836319491660581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimized pipeline was not improved after evaluating 5 more generations. Will end the optimization process.\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.1, max_depth=9, max_features=0.9000000000000001, min_samples_leaf=13, min_samples_split=18, n_estimators=100, subsample=0.6000000000000001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'sklearn.naive_bayes.GaussianNB': {}, 'sklearn.naive_bayes.BernoulliNB': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'fit_prior': [True, False]}, 'sklearn.naive_bayes.MultinomialNB': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'fit_prior': [True, False]}, 'sklearn.tree.DecisionT....3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}}}},\n",
       "        crossover_rate=0.1, cv=10, disable_update_check=False,\n",
       "        early_stop=5, generations=50, max_eval_time_mins=5,\n",
       "        max_time_mins=None, memory=None, mutation_rate=0.9, n_jobs=3,\n",
       "        offspring_size=50, periodic_checkpoint_folder=None,\n",
       "        population_size=50, random_state=None, scoring=None, subsample=1.0,\n",
       "        verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9438202247191011"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train error\n",
    "pipeline_optimizer.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8395522388059702"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test error\n",
    "pipeline_optimizer.score(X_test,y_test)  # result is nearly same as CV result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"EA\\y_hat_test.csv\", pipeline_optimizer.predict(X_test), delimiter=\",\")\n",
    "np.savetxt(\"EA\\y_hat_train.csv\", pipeline_optimizer.predict(X_train), delimiter=\",\")\n",
    "np.savetxt(\"EA\\y_hat_prob_test.csv\", pipeline_optimizer.predict_proba(X_test), delimiter=\",\")\n",
    "np.savetxt(\"EA\\y_hat_prob_train.csv\", pipeline_optimizer.predict_proba(X_train), delimiter=\",\")\n",
    "X_train.to_csv('EA\\X_train.csv')\n",
    "X_test.to_csv('EA\\X_test.csv')\n",
    "y_train.to_csv('EA\\y_train.csv')\n",
    "y_test.to_csv('EA\\y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_optimizer.export('TPOT_result.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Result from TPOT shows that with Random Forests/Boosting Tree, training error is about 6.5%, while CV and test error is about 15~20%.\n",
    "\n",
    "Therefore, the model is suffering high variance problem. To reduce variance, below methods can be considered.\n",
    "\n",
    "1. **more training examples (try 10 folds)**\n",
    "2. smaller sets of features (already small)\n",
    "3. **larger regularization (for tree based algorithm, reduce max_depth, increase n_estimator)**\n",
    "4. less flexible model (should be tested in TPOT)\n",
    "\n",
    "So, next steps:\n",
    "1. Use RandomForestClassifier and ExtraTreesClassifier (potential of getting lower variance)\n",
    "2. Increase regularization by controlling max_depth and n_estimator\n",
    "3. Use GradientBoostingClassifier as reference of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RandomizedSearchCV for RandomForestClassifier\n",
    "\"\"\"\n",
    "Because RF is based on bootstrap resampling, we can evaluate generalization ability\n",
    "by using out-of-bag (OOB) error instead of validation set or cross validation process.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Feature Engineering\n",
    "\n",
    "### Linear correlation between features\n",
    "\n",
    "Randome forest is hard to catch the information of linear correlation between features, while logistic regression is better on it. For example, feature 1 and 2 in dataset\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Basic Feature Engineering with the Titanic Data](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
